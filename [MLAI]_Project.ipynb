{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of [MLAI] Project",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ennTzcMfWbr1",
        "colab_type": "text"
      },
      "source": [
        "**Install requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfcSocyz7xLI",
        "colab_type": "code",
        "outputId": "3ffdecd3-4bb6-499d-8497-ada8a293a94f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!pip3 install 'keras'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.17.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.3.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jz3SXYtGkSl",
        "colab_type": "text"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyWOQ7ceGlkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "from random import shuffle\n",
        "\n",
        "import keras\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "from keras import layers\n",
        "import keras.backend as K\n",
        "from keras.models import Model\n",
        "from keras.utils import plot_model\n",
        "from keras.utils import layer_utils\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
        "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
        "\n",
        "import pydot\n",
        "from IPython.display import SVG\n",
        "\n",
        "K.set_image_data_format('channels_last')\n",
        "\n",
        "from matplotlib.pyplot import imshow\n",
        "from keras.preprocessing import image\n",
        "from keras import applications\n",
        "from keras.models import Sequential\n",
        "import sys\n",
        "\n",
        "np.random.seed(1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bky2gGCc296",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shutil.rmtree('test') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on6-T32ai4Te",
        "colab_type": "text"
      },
      "source": [
        "**Utility Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbXa_Qc5i8lx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "  model = Sequential()\n",
        "  model.add(BatchNormalization(input_shape=(224, 224, 3)))\n",
        "  model.add(Conv2D(filters=16, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=2))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(Conv2D(filters=32, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=2))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(Conv2D(filters=64, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=2))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(Conv2D(filters=128, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=2))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(Conv2D(filters=256, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=2))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(GlobalAveragePooling2D())\n",
        "\n",
        "  model.add(Dense(120, activation='softmax'))\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  return model\n",
        "\n",
        "# def alexnet_model():\n",
        "#   #Instantiate an empty model\n",
        "#   model = Sequential()\n",
        "\n",
        "#   # 1st Convolutional Layer\n",
        "#   model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11), strides=(4,4), padding=’valid’))\n",
        "#   model.add(Activation(‘relu’))\n",
        "#   # Max Pooling\n",
        "#   model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=’valid’))\n",
        "\n",
        "#   # 2nd Convolutional Layer\n",
        "#   model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding=’valid’))\n",
        "#   model.add(Activation(‘relu’))\n",
        "#   # Max Pooling\n",
        "#   model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=’valid’))\n",
        "\n",
        "#   # 3rd Convolutional Layer\n",
        "#   model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding=’valid’))\n",
        "#   model.add(Activation(‘relu’))\n",
        "\n",
        "#   # 4th Convolutional Layer\n",
        "#   model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding=’valid’))\n",
        "#   model.add(Activation(‘relu’))\n",
        "\n",
        "#   # 5th Convolutional Layer\n",
        "#   model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding=’valid’))\n",
        "#   model.add(Activation(‘relu’))\n",
        "#   # Max Pooling\n",
        "#   model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=’valid’))\n",
        "\n",
        "#   # Passing it to a Fully Connected layer\n",
        "#   model.add(Flatten())\n",
        "#   # 1st Fully Connected Layer\n",
        "#   model.add(Dense(4096, input_shape=(224*224*3,)))\n",
        "#   model.add(Activation(‘relu’))\n",
        "#   # Add Dropout to prevent overfitting\n",
        "#   model.add(Dropout(0.4))\n",
        "\n",
        "#   # 2nd Fully Connected Layer\n",
        "#   model.add(Dense(4096))\n",
        "#   model.add(Activation(‘relu’))\n",
        "#   # Add Dropout\n",
        "#   model.add(Dropout(0.4))\n",
        "\n",
        "#   # 3rd Fully Connected Layer\n",
        "#   model.add(Dense(1000))\n",
        "#   model.add(Activation(‘relu’))\n",
        "#   # Add Dropout\n",
        "#   model.add(Dropout(0.4))\n",
        "\n",
        "#   # Output Layer\n",
        "#   model.add(Dense(17))\n",
        "#   model.add(Activation(‘softmax’))\n",
        "\n",
        "#   model.summary()\n",
        "\n",
        "#   return model\n",
        "\n",
        "def compile_model(model, _opt='adam', _loss='categorical_crossentropy', _metrics=['accuracy']):\n",
        "  model.compile(_opt, _loss, _metrics)\n",
        "  return model\n",
        "\n",
        "def set_checkpointer(_filePath):\n",
        "  checkpointer = ModelCheckpoint(filepath=_filePath, verbose=1, \n",
        "                                 save_best_only=True)\n",
        "  return checkpointer\n",
        "\n",
        "def train(model, num_epochs, batch_size, step_size, train_data, train_target, valid_data, valid_target, checkpointer):\n",
        "  model.fit_generator(datagen.flow(train_data, train_target, batch_size=batch_size),\n",
        "                    validation_data=(valid_data, valid_target), \n",
        "                    steps_per_epoch=train_data.shape[0] // batch_size,\n",
        "                    epochs=epochs, callbacks=[checkpointer], verbose=1)\n",
        "\n",
        "def fit_gen(model, _train_set, _steps_per_epoch, _epoch, _valid_set, _valid_steps=800):\n",
        "  model.fit_generator(\n",
        "    _train_set,\n",
        "    steps_per_epoch = _steps_per_epoch,\n",
        "    epochs = _epoch,\n",
        "    validation_data = _valid_set,\n",
        "    validation_steps = _valid_steps)\n",
        "  \n",
        "  return model\n",
        "\n",
        "# def load_best_model(model, _filePath):\n",
        "#   model.load_weights(_filePath)\n",
        "\n",
        "def test(model, test_data, test_target):\n",
        "  dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_data]\n",
        "\n",
        "  test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_target, axis=1))/len(dog_breed_predictions)\n",
        "  print('Test accuracy: %.4f%%' % test_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHiId7xUeJpT",
        "colab_type": "text"
      },
      "source": [
        "**Dataset preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrmQ5xfseM8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone github repository with data\n",
        "if not os.path.isdir('./mlai/Images'):\n",
        "  !git clone https://github.com/jmagdeska/mlai.git\n",
        "\n",
        "DATA_DIR = 'mlai/Images'\n",
        "\n",
        "if not os.path.isdir('train'):\n",
        "  os.mkdir('train')\n",
        "if not os.path.isdir('valid'):\n",
        "  os.mkdir('valid')\n",
        "if not os.path.isdir('test'):\n",
        "  os.mkdir('test')\n",
        "\n",
        "for path, dirs, files in os.walk(DATA_DIR):\n",
        "  dirs.sort(key = lambda x: x.lower())\n",
        "  num_samples = len(files)\n",
        "  i = 0\n",
        "  \n",
        "  l = (int)(0.8*num_samples)\n",
        "  if l != 0:\n",
        "    train_len = int(0.8*l)\n",
        "    valid_len = l - train_len\n",
        "    test_len = num_samples - l\n",
        "\n",
        "    label = path.split(\"/\")[2]\n",
        "    shuffle(files)\n",
        "\n",
        "    for filename in files: \n",
        "      full_path = os.path.join(path, filename)       \n",
        "      if i < train_len:   \n",
        "        split = 'train'     \n",
        "      elif i < (train_len + valid_len):\n",
        "        split = 'valid'\n",
        "      else:\n",
        "        split = 'test'\n",
        "      \n",
        "      dir_name = os.path.join(split, label)\n",
        "      if not os.path.isdir(dir_name):\n",
        "        os.mkdir(os.path.join(split, label))\n",
        "      shutil.move(full_path, dir_name)\n",
        "\n",
        "      i += 1\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P05uaVocgcyy",
        "colab_type": "text"
      },
      "source": [
        "**Dataset and Dataloader Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDLOEt73gsa-",
        "colab_type": "code",
        "outputId": "17550c93-3d02-4edc-8c01-45e6184397bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "TRAIN_DIR = './train'\n",
        "VALID_DIR = './valid'\n",
        "TEST_DIR = './test'\n",
        "\n",
        "datagen_train = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "datagen_test = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "generator_train=datagen_train.flow_from_directory(TRAIN_DIR, target_size=(224,224), batch_size=32)\n",
        "\n",
        "generator_valid=datagen_test.flow_from_directory(VALID_DIR, target_size=(224, 224), batch_size=32)\n",
        "\n",
        "#generator_test=datagen_test.flow_from_directory(TEST_DIR)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 13091 images belonging to 120 classes.\n",
            "Found 3327 images belonging to 120 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VF9HItToMGA",
        "colab_type": "text"
      },
      "source": [
        "**Main**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9JjmJPgoNk5",
        "colab_type": "code",
        "outputId": "129f96de-7578-4edb-8802-ad16b584b09f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "STEP_SIZE = 200\n",
        "NUM_EPOCHS = 30\n",
        "VALID_STEP_SIZE = 80\n",
        "\n",
        "my_model = create_model()\n",
        "my_model = compile_model(my_model)\n",
        "best_model_path = 'saved_models/weights.bestaugmented.from_scratch.hdf5'\n",
        "chPointer = set_checkpointer(best_model_path)\n",
        "my_model = fit_gen(my_model, generator_train, STEP_SIZE, NUM_EPOCHS, generator_valid, VALID_STEP_SIZE)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization_1 (Batch (None, 224, 224, 3)       12        \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 222, 222, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 111, 111, 16)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 111, 111, 16)      64        \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 109, 109, 32)      4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 54, 54, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 54, 54, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 52, 52, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 26, 26, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 26, 26, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 24, 24, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 10, 10, 256)       295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 5, 5, 256)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 5, 5, 256)         1024      \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 120)               30840     \n",
            "=================================================================\n",
            "Total params: 425,444\n",
            "Trainable params: 424,446\n",
            "Non-trainable params: 998\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/30\n",
            "200/200 [==============================] - 97s 485ms/step - loss: 4.6413 - acc: 0.0323 - val_loss: 4.5664 - val_acc: 0.0395\n",
            "Epoch 2/30\n",
            "200/200 [==============================] - 87s 434ms/step - loss: 4.3744 - acc: 0.0517 - val_loss: 4.4907 - val_acc: 0.0594\n",
            "Epoch 3/30\n",
            "200/200 [==============================] - 87s 433ms/step - loss: 4.2043 - acc: 0.0733 - val_loss: 4.2945 - val_acc: 0.0539\n",
            "Epoch 4/30\n",
            "200/200 [==============================] - 86s 429ms/step - loss: 4.0638 - acc: 0.0925 - val_loss: 4.0519 - val_acc: 0.0950\n",
            "Epoch 5/30\n",
            "200/200 [==============================] - 85s 424ms/step - loss: 3.8680 - acc: 0.1131 - val_loss: 4.0831 - val_acc: 0.0879\n",
            "Epoch 6/30\n",
            "200/200 [==============================] - 85s 424ms/step - loss: 3.7908 - acc: 0.1291 - val_loss: 3.9763 - val_acc: 0.1000\n",
            "Epoch 7/30\n",
            "200/200 [==============================] - 84s 421ms/step - loss: 3.6010 - acc: 0.1563 - val_loss: 3.7211 - val_acc: 0.1442\n",
            "Epoch 8/30\n",
            "200/200 [==============================] - 85s 425ms/step - loss: 3.5298 - acc: 0.1667 - val_loss: 3.7112 - val_acc: 0.1368\n",
            "Epoch 9/30\n",
            "200/200 [==============================] - 83s 417ms/step - loss: 3.3629 - acc: 0.1938 - val_loss: 3.6322 - val_acc: 0.1539\n",
            "Epoch 10/30\n",
            "200/200 [==============================] - 84s 421ms/step - loss: 3.3246 - acc: 0.2100 - val_loss: 3.4960 - val_acc: 0.1774\n",
            "Epoch 11/30\n",
            "200/200 [==============================] - 84s 421ms/step - loss: 3.1449 - acc: 0.2347 - val_loss: 3.5583 - val_acc: 0.1762\n",
            "Epoch 12/30\n",
            "200/200 [==============================] - 84s 420ms/step - loss: 3.1192 - acc: 0.2455 - val_loss: 3.4773 - val_acc: 0.1829\n",
            "Epoch 13/30\n",
            "200/200 [==============================] - 84s 419ms/step - loss: 2.9860 - acc: 0.2720 - val_loss: 3.2157 - val_acc: 0.2173\n",
            "Epoch 14/30\n",
            "200/200 [==============================] - 84s 420ms/step - loss: 2.9213 - acc: 0.2802 - val_loss: 3.4011 - val_acc: 0.1824\n",
            "Epoch 15/30\n",
            "200/200 [==============================] - 84s 420ms/step - loss: 2.8027 - acc: 0.3053 - val_loss: 3.1116 - val_acc: 0.2302\n",
            "Epoch 16/30\n",
            "200/200 [==============================] - 84s 419ms/step - loss: 2.7610 - acc: 0.3099 - val_loss: 3.1646 - val_acc: 0.2384\n",
            "Epoch 17/30\n",
            "200/200 [==============================] - 84s 420ms/step - loss: 2.6246 - acc: 0.3409 - val_loss: 3.2661 - val_acc: 0.2200\n",
            "Epoch 18/30\n",
            "200/200 [==============================] - 85s 423ms/step - loss: 2.6041 - acc: 0.3427 - val_loss: 3.2611 - val_acc: 0.2137\n",
            "Epoch 19/30\n",
            "200/200 [==============================] - 84s 419ms/step - loss: 2.5310 - acc: 0.3642 - val_loss: 2.9834 - val_acc: 0.2724\n",
            "Epoch 20/30\n",
            "200/200 [==============================] - 84s 418ms/step - loss: 2.4572 - acc: 0.3798 - val_loss: 2.8610 - val_acc: 0.2888\n",
            "Epoch 21/30\n",
            "200/200 [==============================] - 84s 420ms/step - loss: 2.3801 - acc: 0.4017 - val_loss: 3.0931 - val_acc: 0.2470\n",
            "Epoch 22/30\n",
            "200/200 [==============================] - 84s 419ms/step - loss: 2.2943 - acc: 0.4144 - val_loss: 2.8828 - val_acc: 0.2988\n",
            "Epoch 23/30\n",
            "200/200 [==============================] - 84s 418ms/step - loss: 2.2765 - acc: 0.4134 - val_loss: 2.9486 - val_acc: 0.2814\n",
            "Epoch 24/30\n",
            "200/200 [==============================] - 84s 418ms/step - loss: 2.1905 - acc: 0.4291 - val_loss: 2.9107 - val_acc: 0.2919\n",
            "Epoch 25/30\n",
            "200/200 [==============================] - 84s 420ms/step - loss: 2.1521 - acc: 0.4475 - val_loss: 2.8366 - val_acc: 0.3017\n",
            "Epoch 26/30\n",
            "200/200 [==============================] - 84s 421ms/step - loss: 2.0779 - acc: 0.4613 - val_loss: 2.9209 - val_acc: 0.2946\n",
            "Epoch 27/30\n",
            "200/200 [==============================] - 84s 418ms/step - loss: 2.0326 - acc: 0.4681 - val_loss: 2.8014 - val_acc: 0.3070\n",
            "Epoch 28/30\n",
            "200/200 [==============================] - 85s 426ms/step - loss: 1.9507 - acc: 0.4922 - val_loss: 2.7491 - val_acc: 0.3064\n",
            "Epoch 29/30\n",
            "200/200 [==============================] - 85s 425ms/step - loss: 1.9418 - acc: 0.4904 - val_loss: 2.8018 - val_acc: 0.3220\n",
            "Epoch 30/30\n",
            "200/200 [==============================] - 85s 423ms/step - loss: 1.8692 - acc: 0.5070 - val_loss: 2.7589 - val_acc: 0.3337\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}