{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of [MLAI] Project",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ennTzcMfWbr1",
        "colab_type": "text"
      },
      "source": [
        "**Install requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfcSocyz7xLI",
        "colab_type": "code",
        "outputId": "f3794c40-3ed2-45f0-b3ae-9cdc59c1834a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!pip3 install 'keras'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.17.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jz3SXYtGkSl",
        "colab_type": "text"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyWOQ7ceGlkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import numpy as np\n",
        "from random import shuffle\n",
        "\n",
        "from keras import layers\n",
        "import keras.backend as K\n",
        "from keras import optimizers\n",
        "from keras import applications\n",
        "from keras.models import Model\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.utils import layer_utils\n",
        "from keras.preprocessing import image\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
        "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
        "\n",
        "from keras.applications import vgg16\n",
        "\n",
        "import pydot\n",
        "from IPython.display import SVG\n",
        "from matplotlib.pyplot import imshow\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "np.random.seed(1000)\n",
        "K.set_image_data_format('channels_last')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bky2gGCc296",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shutil.rmtree('test') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on6-T32ai4Te",
        "colab_type": "text"
      },
      "source": [
        "**Utility Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbXa_Qc5i8lx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "  model = Sequential()\n",
        "  model.add(BatchNormalization(input_shape=(224, 224, 3)))\n",
        "  model.add(Conv2D(filters=16, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=2))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(Conv2D(filters=32, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=2))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(Conv2D(filters=64, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=2))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(Conv2D(filters=128, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=2))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(Conv2D(filters=256, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=2))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(GlobalAveragePooling2D())\n",
        "\n",
        "  model.add(Dense(120, activation='softmax'))\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  return model\n",
        "\n",
        "# def alexnet_model():\n",
        "#   #Instantiate an empty model\n",
        "#   model = Sequential()\n",
        "\n",
        "#   # 1st Convolutional Layer\n",
        "#   model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11), strides=(4,4), padding=’valid’))\n",
        "#   model.add(Activation(‘relu’))\n",
        "#   # Max Pooling\n",
        "#   model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=’valid’))\n",
        "\n",
        "#   # 2nd Convolutional Layer\n",
        "#   model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding=’valid’))\n",
        "#   model.add(Activation(‘relu’))\n",
        "#   # Max Pooling\n",
        "#   model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=’valid’))\n",
        "\n",
        "#   # 3rd Convolutional Layer\n",
        "#   model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding=’valid’))\n",
        "#   model.add(Activation(‘relu’))\n",
        "\n",
        "#   # 4th Convolutional Layer\n",
        "#   model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding=’valid’))\n",
        "#   model.add(Activation(‘relu’))\n",
        "\n",
        "#   # 5th Convolutional Layer\n",
        "#   model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding=’valid’))\n",
        "#   model.add(Activation(‘relu’))\n",
        "#   # Max Pooling\n",
        "#   model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=’valid’))\n",
        "\n",
        "#   # Passing it to a Fully Connected layer\n",
        "#   model.add(Flatten())\n",
        "#   # 1st Fully Connected Layer\n",
        "#   model.add(Dense(4096, input_shape=(224*224*3,)))\n",
        "#   model.add(Activation(‘relu’))\n",
        "#   # Add Dropout to prevent overfitting\n",
        "#   model.add(Dropout(0.4))\n",
        "\n",
        "#   # 2nd Fully Connected Layer\n",
        "#   model.add(Dense(4096))\n",
        "#   model.add(Activation(‘relu’))\n",
        "#   # Add Dropout\n",
        "#   model.add(Dropout(0.4))\n",
        "\n",
        "#   # 3rd Fully Connected Layer\n",
        "#   model.add(Dense(1000))\n",
        "#   model.add(Activation(‘relu’))\n",
        "#   # Add Dropout\n",
        "#   model.add(Dropout(0.4))\n",
        "\n",
        "#   # Output Layer\n",
        "#   model.add(Dense(17))\n",
        "#   model.add(Activation(‘softmax’))\n",
        "\n",
        "#   model.summary()\n",
        "\n",
        "#   return model\n",
        "\n",
        "def compile_model(model, _opt='adam', _loss='categorical_crossentropy', _metrics=['accuracy']):\n",
        "  model.compile(_opt, _loss, _metrics)\n",
        "  return model\n",
        "\n",
        "def set_checkpointer(_filePath):\n",
        "  checkpointer = ModelCheckpoint(filepath=_filePath, verbose=1, \n",
        "                                 save_best_only=True)\n",
        "  return checkpointer\n",
        "\n",
        "def train(model, num_epochs, batch_size, step_size, train_data, train_target, valid_data, valid_target, checkpointer):\n",
        "  model.fit_generator(datagen.flow(train_data, train_target, batch_size=batch_size),\n",
        "                    validation_data=(valid_data, valid_target), \n",
        "                    steps_per_epoch=train_data.shape[0] // batch_size,\n",
        "                    epochs=epochs, callbacks=[checkpointer], verbose=1)\n",
        "\n",
        "def fit_gen(model, _train_set, _steps_per_epoch, _epoch, _valid_set, _valid_steps=800):\n",
        "  model.fit_generator(\n",
        "    _train_set,\n",
        "    steps_per_epoch = _steps_per_epoch,\n",
        "    epochs = _epoch,\n",
        "    validation_data = _valid_set,\n",
        "    validation_steps = _valid_steps)\n",
        "  \n",
        "  return model\n",
        "\n",
        "# def load_best_model(model, _filePath):\n",
        "#   model.load_weights(_filePath)\n",
        "\n",
        "def test(model, test_data, test_target):\n",
        "  dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_data]\n",
        "\n",
        "  test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_target, axis=1))/len(dog_breed_predictions)\n",
        "  print('Test accuracy: %.4f%%' % test_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHiId7xUeJpT",
        "colab_type": "text"
      },
      "source": [
        "**Dataset preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrmQ5xfseM8-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "9592e2d5-1c9a-4608-ad1a-fdb455ff5ca3"
      },
      "source": [
        "# Clone github repository with data\n",
        "if not os.path.isdir('./mlai/Images'):\n",
        "  !git clone https://github.com/jmagdeska/mlai.git\n",
        "\n",
        "DATA_DIR = 'mlai/Images'\n",
        "\n",
        "if not os.path.isdir('train'):\n",
        "  os.mkdir('train')\n",
        "if not os.path.isdir('valid'):\n",
        "  os.mkdir('valid')\n",
        "if not os.path.isdir('test'):\n",
        "  os.mkdir('test')\n",
        "\n",
        "for path, dirs, files in os.walk(DATA_DIR):\n",
        "  dirs.sort(key = lambda x: x.lower())\n",
        "  num_samples = len(files)\n",
        "  i = 0\n",
        "  \n",
        "  l = (int)(0.8*num_samples)\n",
        "  if l != 0:\n",
        "    train_len = int(0.8*l)\n",
        "    valid_len = l - train_len\n",
        "    test_len = num_samples - l\n",
        "\n",
        "    label = path.split(\"/\")[2]\n",
        "    shuffle(files)\n",
        "\n",
        "    for filename in files: \n",
        "      full_path = os.path.join(path, filename)       \n",
        "      if i < train_len:   \n",
        "        split = 'train'     \n",
        "      elif i < (train_len + valid_len):\n",
        "        split = 'valid'\n",
        "      else:\n",
        "        split = 'test'\n",
        "      \n",
        "      dir_name = os.path.join(split, label)\n",
        "      if not os.path.isdir(dir_name):\n",
        "        os.mkdir(os.path.join(split, label))\n",
        "      shutil.move(full_path, dir_name)\n",
        "\n",
        "      i += 1  "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'mlai'...\n",
            "remote: Enumerating objects: 31, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 41143 (delta 10), reused 3 (delta 0), pack-reused 41112\u001b[K\n",
            "Receiving objects: 100% (41143/41143), 1.38 GiB | 52.72 MiB/s, done.\n",
            "Resolving deltas: 100% (10/10), done.\n",
            "Checking out files: 100% (20581/20581), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P05uaVocgcyy",
        "colab_type": "text"
      },
      "source": [
        "**Dataset and Dataloader Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDLOEt73gsa-",
        "colab_type": "code",
        "outputId": "4f06ce22-eb33-4b20-f244-16cb44f80c75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "TRAIN_DIR = './train'\n",
        "VALID_DIR = './valid'\n",
        "TEST_DIR = './test'\n",
        "\n",
        "datagen_train = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "datagen_test = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "generator_train = datagen_train.flow_from_directory(TRAIN_DIR, target_size=(224,224), batch_size=32)\n",
        "\n",
        "generator_valid = datagen_test.flow_from_directory(VALID_DIR, target_size=(224, 224), batch_size=32)\n",
        "\n",
        "#generator_test=datagen_test.flow_from_directory(TEST_DIR)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 13091 images belonging to 120 classes.\n",
            "Found 3327 images belonging to 120 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VF9HItToMGA",
        "colab_type": "text"
      },
      "source": [
        "**Main**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9JjmJPgoNk5",
        "colab_type": "code",
        "outputId": "fc4dae1f-1948-4adf-833f-19a2a22b60ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "STEP_SIZE = 200\n",
        "NUM_EPOCHS = 30\n",
        "NUM_CLASSES = 120\n",
        "VALID_STEP_SIZE = 80\n",
        "\n",
        "######## Custom model #########\n",
        "# my_model = create_model()\n",
        "# my_model = compile_model(my_model)\n",
        "# best_model_path = 'saved_models/weights.bestaugmented.from_scratch.hdf5'\n",
        "# chPointer = set_checkpointer(best_model_path)\n",
        "# my_model = fit_gen(my_model, generator_train, STEP_SIZE, NUM_EPOCHS, generator_valid, VALID_STEP_SIZE)\n",
        "\n",
        "######## pretrained VGG16 model ########\n",
        "vgg_model = vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "# Freeze the layers except the last 4 layers\n",
        "for layer in vgg_model.layers[:-4]:\n",
        "    layer.trainable = False\n",
        " \n",
        "my_model = Sequential()\n",
        "# Add the vgg convolutional base model\n",
        "my_model.add(vgg_model)\n",
        " \n",
        "# Add new layers\n",
        "my_model.add(Flatten())\n",
        "my_model.add(Dense(1024, activation='relu'))\n",
        "my_model.add(Dropout(0.5))\n",
        "my_model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
        " \n",
        "my_model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
        "              metrics=['acc'])\n",
        "my_model = fit_gen(my_model, generator_train, STEP_SIZE, NUM_EPOCHS, generator_valid, VALID_STEP_SIZE)\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "200/200 [==============================] - 114s 569ms/step - loss: 4.7859 - acc: 0.0152 - val_loss: 4.5885 - val_acc: 0.0273\n",
            "Epoch 2/30\n",
            "200/200 [==============================] - 110s 549ms/step - loss: 4.5132 - acc: 0.0348 - val_loss: 4.2937 - val_acc: 0.0598\n",
            "Epoch 3/30\n",
            "200/200 [==============================] - 107s 534ms/step - loss: 4.0839 - acc: 0.0778 - val_loss: 3.7173 - val_acc: 0.1172\n",
            "Epoch 4/30\n",
            "200/200 [==============================] - 104s 520ms/step - loss: 3.7483 - acc: 0.1260 - val_loss: 3.4923 - val_acc: 0.1727\n",
            "Epoch 5/30\n",
            "200/200 [==============================] - 108s 540ms/step - loss: 3.3017 - acc: 0.1994 - val_loss: 2.9596 - val_acc: 0.2559\n",
            "Epoch 6/30\n",
            "200/200 [==============================] - 108s 538ms/step - loss: 3.0667 - acc: 0.2370 - val_loss: 2.8073 - val_acc: 0.2868\n",
            "Epoch 7/30\n",
            "200/200 [==============================] - 104s 522ms/step - loss: 2.7056 - acc: 0.3042 - val_loss: 2.6088 - val_acc: 0.3232\n",
            "Epoch 8/30\n",
            "200/200 [==============================] - 106s 531ms/step - loss: 2.5795 - acc: 0.3278 - val_loss: 2.4386 - val_acc: 0.3634\n",
            "Epoch 9/30\n",
            "200/200 [==============================] - 107s 534ms/step - loss: 2.3183 - acc: 0.3858 - val_loss: 2.3862 - val_acc: 0.3707\n",
            "Epoch 10/30\n",
            "200/200 [==============================] - 108s 541ms/step - loss: 2.2798 - acc: 0.3948 - val_loss: 2.3142 - val_acc: 0.3841\n",
            "Epoch 11/30\n",
            "200/200 [==============================] - 105s 524ms/step - loss: 2.0029 - acc: 0.4528 - val_loss: 2.2801 - val_acc: 0.3970\n",
            "Epoch 12/30\n",
            "200/200 [==============================] - 105s 525ms/step - loss: 1.9547 - acc: 0.4690 - val_loss: 2.2355 - val_acc: 0.4131\n",
            "Epoch 13/30\n",
            "200/200 [==============================] - 103s 514ms/step - loss: 1.7920 - acc: 0.5098 - val_loss: 2.1450 - val_acc: 0.4275\n",
            "Epoch 14/30\n",
            "200/200 [==============================] - 104s 520ms/step - loss: 1.7102 - acc: 0.5283 - val_loss: 2.3174 - val_acc: 0.4168\n",
            "Epoch 15/30\n",
            "200/200 [==============================] - 101s 503ms/step - loss: 1.5891 - acc: 0.5552 - val_loss: 2.1798 - val_acc: 0.4205\n",
            "Epoch 16/30\n",
            "200/200 [==============================] - 97s 485ms/step - loss: 1.4850 - acc: 0.5871 - val_loss: 2.2233 - val_acc: 0.4318\n",
            "Epoch 17/30\n",
            "200/200 [==============================] - 99s 495ms/step - loss: 1.3934 - acc: 0.6009 - val_loss: 2.0722 - val_acc: 0.4521\n",
            "Epoch 18/30\n",
            "200/200 [==============================] - 100s 498ms/step - loss: 1.3615 - acc: 0.6152 - val_loss: 2.0325 - val_acc: 0.4648\n",
            "Epoch 19/30\n",
            "200/200 [==============================] - 100s 499ms/step - loss: 1.2394 - acc: 0.6523 - val_loss: 2.2267 - val_acc: 0.4322\n",
            "Epoch 20/30\n",
            "200/200 [==============================] - 97s 485ms/step - loss: 1.1774 - acc: 0.6620 - val_loss: 2.1113 - val_acc: 0.4713\n",
            "Epoch 21/30\n",
            "200/200 [==============================] - 95s 477ms/step - loss: 1.1130 - acc: 0.6815 - val_loss: 2.1978 - val_acc: 0.4615\n",
            "Epoch 22/30\n",
            "200/200 [==============================] - 96s 478ms/step - loss: 1.0362 - acc: 0.6981 - val_loss: 2.1653 - val_acc: 0.4664\n",
            "Epoch 23/30\n",
            "200/200 [==============================] - 95s 476ms/step - loss: 0.9559 - acc: 0.7190 - val_loss: 2.2678 - val_acc: 0.4611\n",
            "Epoch 24/30\n",
            "200/200 [==============================] - 96s 478ms/step - loss: 0.8804 - acc: 0.7458 - val_loss: 2.2804 - val_acc: 0.4654\n",
            "Epoch 25/30\n",
            "200/200 [==============================] - 100s 498ms/step - loss: 0.8779 - acc: 0.7404 - val_loss: 2.2768 - val_acc: 0.4599\n",
            "Epoch 26/30\n",
            "200/200 [==============================] - 102s 511ms/step - loss: 0.8160 - acc: 0.7611 - val_loss: 2.5865 - val_acc: 0.4611\n",
            "Epoch 27/30\n",
            "200/200 [==============================] - 101s 507ms/step - loss: 0.7715 - acc: 0.7709 - val_loss: 2.4214 - val_acc: 0.4555\n",
            "Epoch 28/30\n",
            "200/200 [==============================] - 102s 511ms/step - loss: 0.6982 - acc: 0.7905 - val_loss: 2.6129 - val_acc: 0.4639\n",
            "Epoch 29/30\n",
            "200/200 [==============================] - 98s 490ms/step - loss: 0.6938 - acc: 0.7945 - val_loss: 2.5013 - val_acc: 0.4513\n",
            "Epoch 30/30\n",
            "200/200 [==============================] - 100s 499ms/step - loss: 0.6589 - acc: 0.8073 - val_loss: 2.4335 - val_acc: 0.4853\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}